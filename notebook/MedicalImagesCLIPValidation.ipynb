{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvasic/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/16354 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, AdamW\n",
    "import numpy as np\n",
    "\n",
    "# Dataset Class for Custom Image-Caption Pairs\n",
    "class CustomImageTextDataset(Dataset):\n",
    "    def __init__(self, caption_file, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        # Read image-caption pairs from file\n",
    "        with open(caption_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    image_path = os.path.join(image_dir, image_name + \".jpg\")\n",
    "                    if os.path.exists(image_path):\n",
    "                        self.data.append((image_path, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, caption = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "# Collate Function to Combine Batches\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    return list(images), list(captions)\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = CustomImageTextDataset(\"data/train/radiology/captions.txt\", \"data/train/radiology/images\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = CustomImageTextDataset(\"data/validation/radiology/captions.txt\", \"data/validation/radiology/images\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# Load CLIP Model and Processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "# Set model to training mode\n",
    "clip_model.train()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = AdamW(clip_model.parameters(), lr=5e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning Loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    # Training Phase\n",
    "    clip_model.train()\n",
    "    total_loss = 0\n",
    "    for images, captions in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Process images and text\n",
    "        inputs = clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Get logits\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-to-text similarity\n",
    "        logits_per_text = outputs.logits_per_text    # Text-to-image similarity\n",
    "\n",
    "        # Construct ground truth labels\n",
    "        ground_truth = torch.arange(len(images)).long().to(device)  # [0, 1, 2, ..., batch_size-1]\n",
    "\n",
    "        # Compute loss (Symmetric cross-entropy between image-to-text and text-to-image)\n",
    "        loss_i2t = criterion(logits_per_image, ground_truth)\n",
    "        loss_t2i = criterion(logits_per_text, ground_truth)\n",
    "        loss = (loss_i2t + loss_t2i) / 2\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    clip_model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = clip_processor(text=captions, images=images, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs = clip_model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "\n",
    "            ground_truth = torch.arange(len(images)).long().to(device)\n",
    "            loss_i2t = criterion(logits_per_image, ground_truth)\n",
    "            loss_t2i = criterion(logits_per_text, ground_truth)\n",
    "            loss = (loss_i2t + loss_t2i) / 2\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "# Save the fine-tuned CLIP model\n",
    "clip_model.save_pretrained(\"clip_finetuned\")\n",
    "clip_processor.save_pretrained(\"clip_finetuned\")\n",
    "print(\"Fine-tuned CLIP model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
