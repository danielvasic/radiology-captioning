{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvasic/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/dvasic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Dataset Class\n",
    "class RadiologyDataset(Dataset):\n",
    "    def __init__(self, caption_file, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        with open(caption_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    image_path = os.path.join(image_dir, image_name + \".jpg\")\n",
    "                    if os.path.exists(image_path):\n",
    "                        self.data.append((image_path, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, caption = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "# Collate Function\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    return list(images), list(captions)\n",
    "\n",
    "# Evaluation Function for BLEU, ROUGE, and CIDEr\n",
    "def evaluate_metrics(references, candidates):\n",
    "    # Compute BLEU score\n",
    "    bleu_score = corpus_bleu(references, candidates)\n",
    "\n",
    "    # Compute ROUGE score\n",
    "    rouge = Rouge()\n",
    "    rouge_score, _ = rouge.compute_score({i: [\" \".join(ref)] for i, ref in enumerate(references)},\n",
    "                                         {i: [\" \".join(candidate)] for i, candidate in enumerate(candidates)})\n",
    "\n",
    "    # Compute CIDEr score\n",
    "    cider = Cider()\n",
    "    cider_score, _ = cider.compute_score({i: [\" \".join(ref)] for i, ref in enumerate(references)},\n",
    "                                         {i: [\" \".join(candidate)] for i, candidate in enumerate(candidates)})\n",
    "\n",
    "    return bleu_score, rouge_score, cider_score\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = RadiologyDataset(\"data/train/radiology/captions.txt\", \"data/train/radiology/images\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = RadiologyDataset(\"data/validation/radiology/captions.txt\", \"data/validation/radiology/images\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/32707 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.10.11' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load BLIP Model for Conditional Generation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Set model to training mode\n",
    "blip_model.train()\n",
    "\n",
    "# Define the optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(blip_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    # Training Phase\n",
    "    blip_model.train()\n",
    "    total_loss = 0\n",
    "    for images, captions in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Process images and captions\n",
    "        inputs = blip_processor(images=images, text=captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs = blip_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation Phase\n",
    "    blip_model.eval()\n",
    "    references, candidates = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, ground_truth_captions in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = blip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # Generate captions using BLIP\n",
    "            outputs = blip_model.generate(**inputs)\n",
    "            generated_captions = blip_processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            # Preprocess for BLEU, ROUGE, and CIDEr evaluation\n",
    "            references.extend([[nltk.word_tokenize(caption.lower())] for caption in ground_truth_captions])\n",
    "            candidates.extend([nltk.word_tokenize(gen_caption.lower()) for gen_caption in generated_captions])\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    bleu_score, rouge_score, cider_score = evaluate_metrics(references, candidates)\n",
    "    print(f\"Validation BLEU: {bleu_score:.4f}, ROUGE: {rouge_score:.4f}, CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "blip_model.save_pretrained(\"blip_finetuned\")\n",
    "blip_processor.save_pretrained(\"blip_finetuned\")\n",
    "print(\"Fine-tuned BLIP model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
