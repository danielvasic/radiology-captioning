{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvasic/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/dvasic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/dvasic/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/home/dvasic/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dvasic/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch [1/5] Training:  13%|█▎        | 130/1023 [01:32<08:06,  1.83it/s, Batch Loss=6.07]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Vocabulary Class\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<unk>\"]) for token in tokenized_text]\n",
    "\n",
    "# Dataset Class\n",
    "class RadiologyDataset(Dataset):\n",
    "    def __init__(self, caption_file, image_dir, vocab, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "\n",
    "        with open(caption_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    image_path = os.path.join(image_dir, image_name + \".jpg\")\n",
    "                    if os.path.exists(image_path):\n",
    "                        self.data.append((image_path, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, caption = self.data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = [self.vocab.stoi[\"<start>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<end>\"]]\n",
    "        return image, torch.tensor(caption)\n",
    "\n",
    "# Collate Function\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    padded_captions = torch.zeros(len(captions), max(lengths)).long()\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "\n",
    "    return torch.stack(images, 0), padded_captions, lengths\n",
    "\n",
    "# Additional function to convert indices back to words\n",
    "def decode_caption(caption, vocab):\n",
    "    return [vocab.itos[idx] for idx in caption if idx not in {vocab.stoi[\"<start>\"], vocab.stoi[\"<end>\"], vocab.stoi[\"<pad>\"]}]\n",
    "\n",
    "# Evaluation function using BLEU, ROUGE, and CIDEr\n",
    "def evaluate_model(model, data_loader, vocab, device):\n",
    "    model.eval()\n",
    "    references = []  # List of lists to hold reference sentences\n",
    "    candidates = []  # List to hold candidate sentences\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass through the encoder\n",
    "            features = model.encoder(images)\n",
    "\n",
    "            # Generate captions for the images\n",
    "            outputs = model.decoder.generate(features, vocab, max_length=20)\n",
    "\n",
    "            # Split the outputs into separate captions for each image\n",
    "            batch_size = len(images)\n",
    "            batch_outputs = [outputs[i * 20:(i + 1) * 20] for i in range(batch_size)]\n",
    "\n",
    "            # Decode reference and candidate captions to words\n",
    "            decoded_captions = [decode_caption(caption.tolist(), vocab) for caption in captions]\n",
    "            decoded_outputs = [decode_caption(output, vocab) for output in batch_outputs]\n",
    "\n",
    "            references.extend([[ref] for ref in decoded_captions])\n",
    "            candidates.extend(decoded_outputs)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = corpus_bleu(references, candidates)\n",
    "\n",
    "    # Compute ROUGE score\n",
    "    rouge = Rouge()\n",
    "    rouge_score, _ = rouge.compute_score({i: [\" \".join(ref)] for i, ref in enumerate(references)},\n",
    "                                         {i: [\" \".join(candidate)] for i, candidate in enumerate(candidates)})\n",
    "\n",
    "    # Compute CIDEr score\n",
    "    cider = Cider()\n",
    "    cider_score, _ = cider.compute_score({i: [\" \".join(ref)] for i, ref in enumerate(references)},\n",
    "                                         {i: [\" \".join(candidate)] for i, candidate in enumerate(candidates)})\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}, ROUGE Score: {rouge_score:.4f}, CIDEr Score: {cider_score:.4f}\")\n",
    "    return bleu_score, rouge_score, cider_score\n",
    "\n",
    "# Encoder CNN\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]  # Remove the last fully-connected layer\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "# Decoder RNN\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    # Implement the `generate` method for caption generation\n",
    "    def generate(self, features, vocab, max_length=20):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "\n",
    "        # Initialize the hidden state with zero\n",
    "        states = None\n",
    "        for _ in range(max_length):\n",
    "            hiddens, states = self.lstm(inputs, states)  # Forward pass through LSTM\n",
    "            outputs = self.linear(hiddens.squeeze(1))    # Compute outputs\n",
    "\n",
    "            # Take the word with the maximum probability for each image in the batch\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Append predicted values for each image in the batch\n",
    "            for i in range(predicted.size(0)):\n",
    "                sampled_ids.append(predicted[i].item())\n",
    "\n",
    "            # Embed the predicted word for the next input\n",
    "            inputs = self.embed(predicted)               \n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        \n",
    "        return sampled_ids\n",
    "\n",
    "# Image Captioning Model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "    \n",
    "    def forward(self, images, captions, lengths):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions, lengths)\n",
    "        return outputs\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build the vocabulary\n",
    "freq_threshold = 5\n",
    "captions = []\n",
    "\n",
    "with open(\"data/train/radiology/captions.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            _, caption = parts\n",
    "            captions.append(caption)\n",
    "\n",
    "# Create and build vocabulary\n",
    "vocab = Vocabulary(freq_threshold)\n",
    "vocab.build_vocabulary(captions)\n",
    "\n",
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = RadiologyDataset(\"data/train/radiology/captions.txt\", \"data/train/radiology/images\", vocab, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=16, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = RadiologyDataset(\"data/test/radiology/captions.txt\", \"data/test/radiology/images\", vocab, transform=transform)\n",
    "test_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=16, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab)  # Using the correct vocabulary size\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\")\n",
    "\n",
    "    for i, (images, captions, lengths) in train_progress_bar:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        outputs = model(images, captions, lengths)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {average_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the training set with BLEU, ROUGE, and CIDEr metrics\n",
    "    bleu_score, rouge_score, cider_score = evaluate_model(model, test_loader, vocab, device)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training BLEU: {bleu_score:.4f}, ROUGE: {rouge_score:.4f}, CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"image_captioning_model.pth\")\n",
    "print(\"Model saved as image_captioning_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
